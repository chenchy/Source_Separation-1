# optimizer
learning_rate: 0.001
patience: 80 #80 # wait how many epochs without updating will adjust the learning rate
decay_factor: 0.3 # how many percent decrease for the learning rate each time
weight_decay: 0.00001

# general parameters
gradient_clip_val: 100 # maximum gradient value
seed: 42 # random seed
gpus: 1 # how many paralleled gpu for training
early_stopping_patience: 140 #140 # wait for how many epochs not updating before applying early stopping
batch_size: 16 #16 # numbers of batch
num_workers: 4 # numbers of workers for paralleled dataloader
epochs: 1000 # maximum epochs
load_pretrain: None #'logger/open-unmix_slakh_2/' # pre-trained model version number, None without loading pre-trained model

# data augmentation
aug_gain: True # use random volum adjust
aug_channelswap: True # use channel swap
aug_pitchShift: False # use pitch swift
aug_freqmask: False

# output
save_path: 'logger/' # path to store the output model

# dataset
dataset_name: 'musdb' # which dataset to use: [musdb]
data_path: '../data/MUSDB18-HQ/' #'../data/slakh/midi/' # path of the dataset:
sf2_dir: '../data/sf2/'
target: 'vocals' # which target source to train
seq_dur: 6 # duration of each chunk
samples_per_track: 64 #64 # how many sample per song
use_norm: True
n_channels: 2

# preprocess
preprocess_name: 'stft'
n_fft: 4096
hop_length: 1024
sample_rate: 44100
band_width: 16000

# model_name
model_name: 'unet' # tcn / unet / open-unmix / spleeter
kernal_size: 3 # numbers of kernal, only useful when model_name equals tcn
n_blocks: 8 # numbers of dilated blocks, only useful when model_name equals tcn
n_stacks: 4 # numbers of repeated blocks, only useful when model_name equals tcn
n_features: 512 # numbers of feature bin of the input data
loss_name: 'mse' # mse / l1

# testing/eval
eval_dir: 'eval_log/'


